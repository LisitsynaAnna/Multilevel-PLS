{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.cross_decomposition import PLSCanonical\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import copy\n",
    "import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(X, Y, subj, num_samples, mean,deviation, ddof, method, feature_selection, model_params,\n",
    "               num_features, step, verbose_train, scaling):\n",
    "    #print \"Classifier got: \", X\n",
    "    #print \"Y is: \", Y\n",
    "    #process the labels\n",
    "    if scaling:\n",
    "        Y_temp = np.array(Y)\n",
    "        Y_temp = descale_data(matrix=Y_temp, deviation=deviation, ddof=ddof) #descale\n",
    "        Y_temp = Y_temp+mean                             #add mean\n",
    "        for j in range(len(Y_temp)):\n",
    "            if Y_temp[j]>1:\n",
    "                Y_temp[j]=1 \n",
    "            elif Y_temp[j]<0:\n",
    "                Y_temp[j]=0\n",
    "                Y_temp=Y_temp.astype(int)\n",
    "            else:\n",
    "                Y_temp = Y+mean\n",
    "\n",
    "    #choose, which classifier to use\n",
    "    if method== 'rf':#build a generic Random Forest\n",
    "        model = RandomForestRegressor(n_estimators=model_params['n_trees'], random_state=0, \n",
    "                                      max_features=model_params['max_features'], max_depth=model_params['max_depth'], \n",
    "                                      min_samples_leaf=model_params['min_samples_leaf'])\n",
    "    elif method == 'pls':\n",
    "        model = PLSRegression(n_components=model_params['n_comp'], scale=False) #initialize a generic PLS model\n",
    "        \n",
    "    elif method == 'svm':\n",
    "        model = SVR(C= model_params['C'], epsilon=model_params['epsilon'], kernel=model_params['kernel'], \n",
    "                   gamma = model_params['gamma'], degree=model_params['degree'])\n",
    "    elif method == 'lda':\n",
    "        model = LinearDiscriminantAnalysis()\n",
    "    if method == 'nn':\n",
    "        from keras.wrappers.scikit_learn import KerasRegressor\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Dense, Activation\n",
    "        import tensorflow\n",
    "        coeff = np.zeros(X.shape[1])\n",
    "        features = [True for f in range(X.shape[1])]\n",
    "        estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "        model =estimator\n",
    "        sfold = StratifiedKFold(n_splits=15)\n",
    "        results = cross_val_score(estimator, X, Y_temp, cv=sfold)\n",
    "        estimator.fit(X, Y_temp)\n",
    "        print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "        auc = 0\n",
    "        train_error = 0\n",
    "    else:\n",
    "        #choose a method for feature selection\n",
    "        if feature_selection == 'FromModel':\n",
    "            model_temp = copy.copy(model)\n",
    "            if method=='svm' and model_params['kernel'] != 'linear':\n",
    "                model_temp = SVR(C= 1, epsilon=1, kernel='linear')\n",
    "            model_temp.fit(X, Y)\n",
    "            model_temp = SelectFromModel(model, prefit=True)\n",
    "            features = model_temp.get_support()\n",
    "        elif feature_selection == 'rfecv':\n",
    "            rfe = RFECV(estimator=model, cv=20, step=step)\n",
    "            if method=='svm' and model_params['kernel'] != 'linear':\n",
    "                rfe = SVR(C= 1, epsilon=1, kernel='linear')\n",
    "            fit = rfe.fit(X, Y)\n",
    "            features = fit.support_\n",
    "        elif feature_selection == 'rfe':\n",
    "            model_temp = copy.copy(model)\n",
    "            if method=='svm' and model_params['kernel'] != 'linear':\n",
    "                model_temp = SVR(C= 1, epsilon=1, kernel='linear')\n",
    "            model_temp.fit(X, Y)\n",
    "            rfe = RFE(estimator=model, n_features_to_select=num_features, step=step)\n",
    "            fit = rfe.fit(X, Y)\n",
    "            features = fit.support_\n",
    "        elif feature_selection == 'pca':      \n",
    "            pca = PCA(n_components = model_params['n_comp'])\n",
    "            fit = pca.fit(X)\n",
    "            features = (-np.mean(fit.components_, axis=0)).argsort()[:num_features]\n",
    "        elif feature_selection == 'lda':\n",
    "            clf = LinearDiscriminantAnalysis()\n",
    "            clf.fit(X, Y_temp)\n",
    "            features = clf.coef_.argsort()[:num_features]\n",
    "        else:# feature_selection is None:\n",
    "            features = [True for f in range(X.shape[1])]\n",
    "\n",
    "        #transform input to suit new dimensions\n",
    "        X_new = X[:, features]\n",
    "        if len(X_new.shape) ==3:\n",
    "            X_new =  X_new.reshape(X_new.shape[0], X_new.shape[2])\n",
    "\n",
    "        #fit the model  \n",
    "        if method=='lda':\n",
    "            model.fit(X_new, Y_temp)\n",
    "        elif method:\n",
    "            model.fit(X_new, Y) #ACTUALLY getting the classifier model, fit model to data\n",
    "            pred = model.predict(X_new) #predict the values on the train set\n",
    "\n",
    "        #get coefficients\n",
    "        if method == 'rf':\n",
    "              coeff = model.feature_importances_.flatten()\n",
    "        elif method == 'pls':\n",
    "              coeff = model.coef_.flatten()\n",
    "        elif method == 'svm': #and model_params['kernel']=='linear') :\n",
    "              coeff = model.coef_.flatten()\n",
    "        else:\n",
    "              coeff = np.zeros(X.shape[1])\n",
    "\n",
    "        #process the predictions\n",
    "        pred_temp = pred.flatten()\n",
    "        if feature_selection != 'lda' and scaling:\n",
    "            pred_temp = descale_data(matrix=pred_temp, deviation=deviation, ddof=ddof ) #descale\n",
    "            pred_temp = pred_temp+mean #add mean\n",
    "            pred_temp = np.rint(pred_temp)\n",
    "        elif feature_selection != 'lda':\n",
    "            pred_temp = pred_temp+mean\n",
    "\n",
    "        diff = Y_temp - pred_temp\n",
    "        train_error = sum(diff*diff) #calculate square error\n",
    "        train_error_temp = mean_squared_error(y_true=Y_temp, y_pred=pred_temp)\n",
    "        fpr, tpr, auc = get_roc_auc(labels=Y_temp, predictions=pred_temp)#AUC if needed\n",
    "\n",
    "        if verbose_train:\n",
    "            print \"Training error: \", train_error/num_samples\n",
    "            print \"Training error computed in library: \", train_error_temp\n",
    "            print \"Training auc: \", auc\n",
    "            plot_roc_curve(fpr, tpr, auc)\n",
    "    \n",
    "    return features, model, train_error, auc, coeff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_classifier(X, Y, subj, num_samples, mean,deviation, ddof, method, feature_selection, model_params,\n",
    "               num_features, step, verbose_train, scaling):\n",
    "    if scaling:\n",
    "        Y_temp = np.array(Y)\n",
    "        Y_temp = descale_data(matrix=Y_temp, deviation=deviation, ddof=ddof) #descale\n",
    "        Y_temp = Y_temp+mean                             #add mean\n",
    "        for j in range(len(Y_temp)):\n",
    "            if Y_temp[j]>1:\n",
    "                Y_temp[j]=1 \n",
    "            elif Y_temp[j]<0:\n",
    "                Y_temp[j]=0\n",
    "                Y_temp=Y_temp.astype(int)\n",
    "            else:\n",
    "                Y_temp = Y+mean\n",
    "    model = PLSRegression(n_components=model_params['n_comp'], scale=False) #initialize a generic PLS model\n",
    "    model_temp = copy.copy(model)\n",
    "    model_temp.fit(X, Y)\n",
    "    rfe = RFE(estimator=model_temp, n_features_to_select=num_features, step=step)\n",
    "    fit = rfe.fit(X, Y)\n",
    "    features = fit.support_\n",
    "    X_new = X[:, features]\n",
    "    model.fit(X_new, Y) #ACTUALLY getting the classifier model, fit model to data\n",
    "    pred = model.predict(X_new) #predict the values on the train set\n",
    "    pred_temp = pred.flatten()\n",
    "    if scaling:\n",
    "        pred_temp = descale_data(matrix=pred_temp, deviation=deviation, ddof=ddof ) #descale\n",
    "        pred_temp = pred_temp+mean #add mean\n",
    "        pred_temp = np.rint(pred_temp)\n",
    "    pred_temp = pred_temp+mean\n",
    "    diff = Y_temp - pred_temp\n",
    "    train_error = sum(diff*diff) #calculate square error\n",
    "    train_error_temp = mean_squared_error(y_true=Y_temp, y_pred=pred_temp)\n",
    "    fpr, tpr, auc = get_roc_auc(labels=Y_temp, predictions=pred_temp)#AUC if needed\n",
    "    return features, model, train_error, auc, coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_data(matrix, mean=None):\n",
    "    if mean is None:\n",
    "        matrix = matrix - np.mean(a=matrix, axis=0)\n",
    "    else:\n",
    "        matrix = matrix - mean\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(matrix, deviation=None, ddof=1):\n",
    "    if deviation is None:\n",
    "        deviation= np.std(a=matrix, axis=0, ddof=ddof)\n",
    "    matrix_temp = copy.copy(matrix)\n",
    "    if np.isnan(matrix).any():\n",
    "        matrix = matrix_temp\n",
    "    elif np.count_nonzero(deviation)==0:\n",
    "        matrix = matrix_temp\n",
    "    else:\n",
    "        matrix = matrix/deviation\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descale_data(matrix, deviation, ddof):\n",
    "    if deviation is None:#use the deviation from the same matrix to scale\n",
    "        matrix = matrix*np.std(a=matrix, axis=0, ddof=ddof)\n",
    "    else:#use deviation given in parameters\n",
    "        matrix = matrix*deviation\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_auc(labels, predictions):\n",
    "    #compute precision-recall curve\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true=labels, y_score=predictions, drop_intermediate=False) \n",
    "    #compute area under the curve for this run\n",
    "    auc = metrics.roc_auc_score(y_true=labels, y_score=predictions, average='macro', sample_weight=None)\n",
    "    return fpr, tpr, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, auc):\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MULTILEVEL PLS\n",
    "def perform_multilevel_pls(X, Y, subj, unique_subj, num_unique_subj, num_subj, scaling, par, ddof, method, \n",
    "                           feature_selection, num_features, step, model_params, verbose_train):\n",
    "    \n",
    "    Xb= np.zeros(X.shape) \n",
    "    Xw= np.zeros(X.shape)\n",
    "    #mean-centering\n",
    "    X_centered = center_data(matrix=X, mean=None) #mean centering of data\n",
    "    Y_centered = center_data(matrix=Y, mean=None) \n",
    "\n",
    "    #split matrix into between and within subject variations\n",
    "    if par!='all':\n",
    "        Xb, Xw = split_between_within_subject_matrix(X=X_centered, subj=subj, unique_subj=unique_subj, \n",
    "                                                 num_unique_subj=num_unique_subj, num_subj=num_subj)\n",
    "    \n",
    "    #scaling (if necessary and specified)\n",
    "    if scaling: #scale the data, if the flag is true\n",
    "        X_scaled = scale_data(matrix=X_centered, deviation=None, ddof=ddof)\n",
    "        Y_scaled = scale_data(matrix=Y_centered, deviation=None, ddof=ddof)                      \n",
    "        if par!='all':\n",
    "            Xb_scaled = scale_data(matrix=Xb, deviation=None, ddof=ddof)\n",
    "            Xw_scaled = scale_data(matrix=Xw, deviation=None, ddof=ddof)\n",
    "    else:\n",
    "        X_scaled = X_centered\n",
    "        Y_scaled = Y_centered\n",
    "        if par!='all':\n",
    "            Xb_scaled = Xb\n",
    "            Xw_scaled = Xw\n",
    "    \n",
    "    #which matrix are we interested in\n",
    "    if par=='all':\n",
    "        X_target = X_scaled\n",
    "    elif par=='between':\n",
    "        X_target = Xb_scaled\n",
    "    elif par=='within':\n",
    "        X_target = Xw_scaled\n",
    "    \n",
    "    #perform classifier (PLS or other) on target data\n",
    "    features, model, train_error, train_auc, coeff= classifier(X=X_target, Y=Y_scaled, subj=subj, num_samples=num_subj,\n",
    "                                                           mean=np.mean(Y), deviation= np.std(Y, axis=0, ddof=ddof), \n",
    "                                                           ddof=ddof, method=method, \n",
    "                                                           feature_selection=feature_selection, \n",
    "                                                           num_features=num_features, step=step, \n",
    "                                                           model_params=model_params, verbose_train=verbose_train, \n",
    "                                                           scaling=scaling)\n",
    "    \n",
    "    return features, model, train_error, train_auc, Xb, Xw, Y_scaled, X_scaled, coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_train_test(X, Y, subj, train_subj, test_subj, num_subj):\n",
    "    X_train = None #initialization\n",
    "    X_test = None\n",
    "    Y_train = []\n",
    "    Y_test = []\n",
    "    subj_train = []\n",
    "    subj_test = []  \n",
    "    \n",
    "    #create the test and train dataset from matrix X with chosen subjects\n",
    "    mask = np.isin(subj,train_subj)\n",
    "    inverse_mask = np.invert(mask)\n",
    "\n",
    "    subj_train = subj[mask]\n",
    "    subj_test = subj[inverse_mask]\n",
    "    X_train = X[mask]\n",
    "    Y_train = Y[mask]\n",
    "    X_test = X[inverse_mask]\n",
    "    Y_test = Y[inverse_mask]\n",
    "\n",
    "    num_train_subj = len(subj_train)          #how many entries in train dataset\n",
    "    num_test_subj= num_subj - num_train_subj  #how many entries in test dataset\n",
    "        \n",
    "    return X_train, X_test, Y_train, Y_test, subj_train, subj_test, num_train_subj, num_test_subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_script(num_folds, num_repeats, scaling, num_permutations, par, filename, verbose, ddof, method, \n",
    "                feature_selection,num_features,step, model_params,verbose_train, mode):\n",
    "    \n",
    "    #READ OR MAKE UP DATA\n",
    "    X, Y, subj, IDs, NetCalc, frQ = read_data(filename, mode)\n",
    "    \n",
    "    #create list of unique subjects\n",
    "    unique_subj = np.unique(subj) \n",
    "    #the number of entries = number of subjects corresponding to entries (1 subject per entry)\n",
    "    num_subj = len(subj)    \n",
    "    #the number of unique subjects\n",
    "    num_unique_subj = len(unique_subj) \n",
    "    #initialization of errors\n",
    "    error = 0 \n",
    "    permutation_error=None\n",
    "    permutation_auc=None \n",
    "    permutation_Q=None\n",
    "    full_train_error=0\n",
    "    full_train_auc=0\n",
    "    crossval_error=0\n",
    "    crossval_auc=0\n",
    "    crossval_Q=0\n",
    "    crossval_train_err=0\n",
    "    crossval_train_auc=0\n",
    "    \n",
    "\n",
    "    #PERFORMING MULTILEVEL PLS ON WHOLE DATASET\n",
    "\n",
    "    features, full_model, full_train_error, full_train_auc, Xb, Xw, Y_scaled, X_scaled, coeff= perform_multilevel_pls(X=X, Y=Y, subj=subj, \n",
    "                           unique_subj=unique_subj, num_subj=num_subj, \n",
    "                           num_unique_subj=num_unique_subj, scaling=scaling, par=par, \n",
    "                           ddof=ddof, method=method, feature_selection=feature_selection, \n",
    "                           num_features=num_features, step=step, model_params=model_params, verbose_train=verbose_train)\n",
    "\n",
    "    print \"labels are: \", Y\n",
    "    if method=='pls':\n",
    "        x =  np.matmul(X_scaled[:,features],full_model.x_weights_[:, 0])\n",
    "        y =  np.matmul(X_scaled[:,features],full_model.x_weights_[:, 1])\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.xlabel('PLS component 1')\n",
    "        plt.ylabel('PLS component 2')\n",
    "        colors = ['green', 'orange']\n",
    "        ax.scatter(x,y,c=Y)\n",
    "\n",
    "        fig.savefig(mode+'.png')   # save the figure to file\n",
    "        plt.close(fig)   \n",
    "        \n",
    "    #print \"Features chosen: \", IDs[features]\n",
    "    #print \"Biomarkers chosen: \", NetCalc[features]\n",
    "    #print \"rfQ chosen: \", frQ[features]\n",
    "    #print \"Coefficients: \", coeff\n",
    "    \n",
    "    IDs = np.array(IDs)\n",
    "    coefficients = {}\n",
    "    for id in IDs:\n",
    "        coefficients[id]=0\n",
    "    for i in range(len(IDs[features])):\n",
    "        coefficients[IDs[i]]=coeff[i]\n",
    "\n",
    "    if verbose: \n",
    "        print \"CROSS_VALIDATION ON ACTUAL DATA: \"\n",
    "        \n",
    "    #CROSS-VALIDATION\n",
    "    crossval_error, crossval_auc, crossval_Q, crossval_train_err, crossval_train_auc =    cross_validation(X=X, Y=Y, subj=subj, unique_subj=unique_subj, num_subj=num_subj, num_unique_subj=num_unique_subj, \n",
    "                     num_folds=num_folds, num_repeats=num_repeats, scaling=scaling, par=par, \n",
    "                     verbose=verbose, ddof=ddof, method=method, feature_selection=feature_selection, \n",
    "                     num_features = num_features, step=step, model_params=model_params, verbose_train=verbose_train)\n",
    "  \n",
    "    if verbose: \n",
    "        print \"PERMUTATED DATA CROSS_VALIDATION: \"\n",
    "        \n",
    "    #PERMUTATE\n",
    "    permutation_error, permutation_auc, permutation_Q, permutation_train_error, permutation_train_auc =                                                        validate_permutation(X=X, Y=Y, subj=subj,\n",
    "                                                                             unique_subj=unique_subj, \n",
    "                                                                             num_subj=num_subj,\n",
    "                                                                             num_unique_subj=num_unique_subj,\n",
    "                                                                             num_folds=num_folds,\n",
    "                                                                             num_repeats=num_repeats, \n",
    "                                                                             scaling=scaling,\n",
    "                                                                             num_permutations=num_permutations, \n",
    "                                                                             par=par, verbose=verbose, \n",
    "                                                                             ddof=ddof,method=method,\n",
    "                                                                             feature_selection=feature_selection, \n",
    "                                                                             num_features=num_features, step=step,\n",
    "                                                                             model_params=model_params, \n",
    "                                                                             verbose_train=verbose_train)\n",
    "   \n",
    "    results = {'num_folds':num_folds,'num_repeats':num_repeats,'scaling':scaling,'num_permutations':num_permutations,\n",
    "               'par':par, 'filename':filename, 'verbose':verbose, 'ddof':ddof, 'method':method, \n",
    "               'feature_selection':feature_selection, 'num_features':num_features, 'step':step, \n",
    "               'n_trees':model_params['n_trees'], 'max_depth':model_params['max_depth'], \n",
    "               'max_features':model_params['max_features'], 'min_samples_leaf':model_params['min_samples_leaf'],\n",
    "               'num_comp':model_params['n_comp'], 'full_train_error':full_train_error, 'full_train_auc':full_train_auc, \n",
    "               'crossval_error':crossval_error, 'crossval_auc':crossval_auc, 'crossval_Q':crossval_Q, \n",
    "               'crossval_train_err': crossval_train_err, 'crossval_train_auc':crossval_train_auc, \n",
    "               'permutation_error': permutation_error, 'permutation_auc':permutation_auc, 'permutation_Q':permutation_Q, \n",
    "               'C': model_params['C'], 'gamma': model_params['gamma'], 'epsilon': model_params['epsilon'], \n",
    "               'degree': model_params['degree'], 'kernel': model_params['kernel']}\n",
    "    \n",
    "    return results, coefficients.values(), IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CROSS-VALIDATION\n",
    "def cross_validation(X, Y, subj, unique_subj, num_subj, num_unique_subj, num_folds, num_repeats, scaling, par, \n",
    "                     verbose, ddof, method, feature_selection, num_features, step, model_params,\n",
    "                     verbose_train):\n",
    "    #initialization\n",
    "    error       = 0 \n",
    "    auc         = 0\n",
    "    train_error = 0\n",
    "    train_auc   = 0\n",
    "    \n",
    "    #create array of labels for each subject\n",
    "    Y_temp = [Y[np.where(subj==unique_subj[i])[0][0]] for i in range(num_unique_subj)]\n",
    "    Y=np.array(Y)\n",
    "    \n",
    "    #repeat cross_validation as many times as specified\n",
    "    for i in range(num_repeats): \n",
    "        kf = StratifiedKFold(n_splits=num_folds)\n",
    "        \n",
    "        for train_index, test_index in kf.split(X[:num_unique_subj], Y_temp):#repeat with every fold as test set\n",
    "            train_subj = unique_subj[train_index]\n",
    "            test_subj = unique_subj[test_index]\n",
    "        \n",
    "            X_train, X_test, Y_train, Y_test, subj_train, subj_test, num_train_subj, num_test_subj = separate_train_test(X=X, Y=Y, subj=subj, train_subj=train_subj,\n",
    "                                                                        test_subj=test_subj, num_subj=num_subj)\n",
    "\n",
    "            num_unique_train_subj=len(train_subj)\n",
    "            num_unique_test_subj=len(test_subj)\n",
    "            \n",
    "            X_train_mean = np.mean(X_train, axis=0)\n",
    "            Y_train_mean = np.mean(Y_train, axis=0)\n",
    "\n",
    "            features, model, train_error_temp, train_auc_temp, Xb_train, Xw_train, Y_scaled_train, X_scaled_train, coeff = perform_multilevel_pls(X=X_train, Y=Y_train, subj=subj_train, \n",
    "                                                              unique_subj=train_subj, \n",
    "                                                              num_unique_subj=num_unique_train_subj, \n",
    "                                                              num_subj=num_train_subj, scaling=scaling, par=par, \n",
    "                                                              ddof=ddof, method=method, \n",
    "                                                              feature_selection=feature_selection, \n",
    "                                                              num_features=num_features, step=step,\n",
    "                                                              model_params=model_params, \n",
    "                                                              verbose_train=verbose_train)\n",
    "\n",
    "            X_centered_test = center_data(X_test, X_train_mean) #mean centering of data\n",
    "            Y_centered_test = center_data(Y_test, Y_train_mean)   \n",
    "\n",
    "            #split test data into between and within subject variation\n",
    "            Xb_test, Xw_test = split_between_within_subject_matrix(X=X_centered_test, subj=subj_test, \n",
    "                                                                   unique_subj=test_subj,\n",
    "                                                                   num_unique_subj=num_unique_test_subj, \n",
    "                                                                   num_subj=num_test_subj )\n",
    "\n",
    "                \n",
    "            if scaling: #scale the data, if the flag is true\n",
    "                X_train_deviation = np.std(X_train, axis = 0, ddof=ddof)\n",
    "                Y_train_deviation = np.std(Y_train, axis = 0, ddof=ddof)\n",
    "                Xb_train_deviation = np.std(Xb_train, axis = 0, ddof=ddof)\n",
    "                Xw_train_deviation = np.std(Xw_train, axis = 0, ddof=ddof)\n",
    "\n",
    "                X_scaled_test = scale_data(matrix=X_centered_test, deviation=X_train_deviation, ddof=ddof)\n",
    "                Y_scaled_test = scale_data(matrix=Y_centered_test, deviation=Y_train_deviation, ddof=ddof)\n",
    "\n",
    "                Xb_scaled_test = scale_data(matrix=Xb_test, deviation=Xb_train_deviation, ddof=ddof)\n",
    "                Xw_scaled_test = scale_data(matrix=Xw_test, deviation=Xw_train_deviation, ddof=ddof)\n",
    "            else:\n",
    "\n",
    "                X_scaled_test = X_centered_test\n",
    "                Y_scaled_test = Y_centered_test\n",
    "                Xb_scaled_test = Xb_test\n",
    "                Xw_scaled_test = Xw_test\n",
    "\n",
    "            #if only between or only within subject variations chosen, predict on that part of the matrix\n",
    "            if par=='all':\n",
    "                X_target = X_scaled_test\n",
    "            elif par=='between':\n",
    "                X_target = Xb_scaled_test\n",
    "            elif par=='within':\n",
    "                X_target = Xw_scaled_test\n",
    "            X_target = X_target[:, features]\n",
    "\n",
    "            if len(X_target.shape)==3:   \n",
    "                X_target = X_target.reshape(X_target.shape[0], X_target.shape[2]) \n",
    "\n",
    "            pred = model.predict(X_target) #predict test data with model trained on the training set\n",
    "             \n",
    "            #process the predicted values\n",
    "            pred=pred.flatten()\n",
    "            pred= pred.reshape(np.product(pred.shape),)\n",
    "            pred = descale_data(matrix=pred, deviation=Y_train_deviation, ddof=ddof)\n",
    "            pred = pred+Y_train_mean\n",
    "\n",
    "            #compute the cross-validation cumulative error (squeared error)\n",
    "            diff = np.array(Y_test - pred) \n",
    "            err_temp= sum(diff*diff)\n",
    "\n",
    "            #get and plot ROC curve, if want\n",
    "            fpr, tpr, auc_temp = get_roc_auc(labels=Y_test, predictions=pred)        \n",
    "            if i%10==0 and verbose:\n",
    "                plot_roc_curve(fpr=fpr, tpr=tpr, auc=auc_temp)\n",
    "                \n",
    "            auc = auc + auc_temp            \n",
    "            error = error + err_temp #compute square error\n",
    "            train_error =train_error+train_error_temp\n",
    "            train_auc = train_auc+train_auc_temp\n",
    "    \n",
    "        \n",
    "    error = float(error)/(num_repeats*num_subj) #mean error for cross-validation\n",
    "    auc = auc/(num_repeats*num_folds)     #mean AUC score for cross validation\n",
    "    train_error = float(train_error)/(num_repeats*num_subj)\n",
    "    train_auc = float(train_auc)/(num_repeats*num_folds) \n",
    "    Q = 1 - error/(sum(Y*Y)/float(num_subj))\n",
    "    if verbose:\n",
    "        print \"Mean cross-validation error: \", error\n",
    "        print \"Mean cross-validation AUC: \", auc\n",
    "        print \"Cross-validation Q: \", Q\n",
    "    if verbose_train:\n",
    "        print \"Mean train cross-validation error: \", error\n",
    "        print \"Mean train cross-validation AUC: \", auc\n",
    "        \n",
    "    return error, auc, Q, train_error, train_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_between_within_subject_matrix(X, subj, unique_subj, num_unique_subj, num_subj): #SPLIT MATRIX \n",
    "    Xb = np.zeros(X.shape) #initialization\n",
    "    Xw = np.zeros(X.shape)\n",
    "    means = np.zeros((num_unique_subj, X.shape[1]))\n",
    "    for j in range(num_unique_subj): #go through all unique subjects\n",
    "        #find indexes of all entries for a certain subject (unique_subj[j])\n",
    "        idx = np.where(np.array(subj)==unique_subj[j]) \n",
    "        #calculate mean for each subject unique_subj[j]      \n",
    "        means[j] = np.mean(X[idx[0]], axis=0)     \n",
    "    for i in range(num_subj): #go through subjects of all entries\n",
    "        #find the index of the subject corresponding to subj[i] in unique_subj\n",
    "        k = np.where(unique_subj==subj[i])\n",
    "        #create a matrix where all entries for subject = mean (between subject variation) \n",
    "        Xb[i] = means[k[0][0]]                  \n",
    "    Xw = X - Xb #get the within subjects matrix             \n",
    "    return Xb, Xw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMUTATION\n",
    "def validate_permutation(X, Y, subj, unique_subj, num_subj, num_unique_subj, num_folds, num_repeats, scaling, \n",
    "                         num_permutations, par, verbose, ddof, method, feature_selection, num_features, \n",
    "                         step, model_params, verbose_train):\n",
    "    \n",
    "    err       = [] #initialization\n",
    "    auc       = []\n",
    "    Q         = []\n",
    "    train_err = []\n",
    "    train_auc = []\n",
    "    \n",
    "    for i in range(num_permutations):#perform permutations as many times as specified\n",
    "        Y_temp = Y.copy()  #create temporary labels which then shuffle/permutate, so original is left untouched\n",
    "        \n",
    "        np.random.shuffle(Y_temp) #randomly shuffle the Y(labels) vector. Checks if your results will be similar\n",
    "        \n",
    "        #perform cross-validation for each permutation and get an array of accuracies when permutated\n",
    "        err_temp, auc_temp, Q_temp, train_error_temp, train_auc_temp  = cross_validation(X=X, Y=Y_temp, subj=subj, \n",
    "                                                                            unique_subj=unique_subj,\n",
    "                                                                            num_subj=num_subj, \n",
    "                                                                            num_unique_subj=num_unique_subj, \n",
    "                                                                            num_folds=num_folds, \n",
    "                                                                            num_repeats=num_repeats, \n",
    "                                                                            scaling=scaling, \n",
    "                                                                            par=par, \n",
    "                                                                            verbose=verbose,ddof=ddof,method=method, \n",
    "                                                                            feature_selection=feature_selection, \n",
    "                                                                            num_features=num_features, step=step, \n",
    "                                                                            model_params=model_params, \n",
    "                                                                            verbose_train=verbose_train)\n",
    "        \n",
    "        #save all the error and metrics values\n",
    "        err.append(err_temp)\n",
    "        auc.append(auc_temp)\n",
    "        Q.append(Q_temp)\n",
    "        train_err.append(train_err_temp)\n",
    "        train_auc.append(train_auc_temp)\n",
    "        \n",
    "    if len(err) >0 and verbose :\n",
    "        print \"Mean permutated squared error : \", np.mean(err)\n",
    "        print \"Mean permutated auc : \", np.mean(auc)\n",
    "        print \"Mean permutated Q: \", np.mean(Q)\n",
    "    elif verbose:\n",
    "        print \"No permutations performed\"\n",
    "    return err, auc, Q, train_err, train_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name=None, mode='delta1'):\n",
    "    if file_name is None: #if no input file specified, make up data\n",
    "        num_subj = 30\n",
    "        num_feat = 1000\n",
    "        X_out = np.random.rand(num_subj, num_feat)\n",
    "        for i in range(3*num_subj):\n",
    "            if i%2==0:\n",
    "                X[i, 0] = X[i, 0]+(np.random.rand()+0.75)*15\n",
    "                X[i, 3] = X[i, 3]+(np.random.rand()+0.75)*20\n",
    "                X[i, 6] = X[i, 6]+(np.random.rand()+0.75)*17\n",
    "        Y =    [1 if i%2 == 0 else 0 for i in range(3*num_subj)]\n",
    "        subjects = [1+i%num_subj for i in range(3*num_subj)]\n",
    "\n",
    "    else:\n",
    "        data = pd.read_excel(file_name) #read the excel file\n",
    "      \n",
    "        X = {}\n",
    "        subjects = {}\n",
    "        Y = {}\n",
    "        \n",
    "        #take the matabolite matrix, transpose, convert to int\n",
    "        X['all'] = data.values[7:, :-9].transpose().astype(np.int64)  \n",
    "        #take relevant values as labels, convert to int\n",
    "        Y['all'] = (data.head().iloc[-3].values[0:148]=='risk').astype(np.int64)\n",
    "        #take relevant values as subject id's, convert to int\n",
    "        subjects['all'] = data.head().iloc[-2].values[0:148].astype(np.int64)\n",
    "        \n",
    "        unique_subj = np.unique(subjects['all']) \n",
    "        num_unique_subj = len(unique_subj)\n",
    "        num_subj = len(subjects['all'])\n",
    "        subj_time = {}\n",
    "        mask_temp = np.array([subjects['all'][index+1]-subjects['all'][index] for index in range(num_subj-1)])\n",
    "        lines = np.where(mask_temp<0)[0]\n",
    "        X['delta1'] = []\n",
    "        X['delta2'] = []\n",
    "        X['delta3'] = []\n",
    "        subjects['delta1'] = []\n",
    "        subjects['delta2'] = []\n",
    "        subjects['delta3'] = []\n",
    "        Y['delta1'] = []\n",
    "        Y['delta2'] = []\n",
    "        Y['delta3'] = []\n",
    "        for s in unique_subj:\n",
    "            indeces_temp = np.where(subjects['all']==s)[0]\n",
    "            subj_time[s]= {'t0':None, 't1':None, 't2':None}\n",
    "            for el in indeces_temp:\n",
    "                    if el>lines[1]:\n",
    "                        subj_time[s]['t2']= el\n",
    "                    elif el <= lines[0]:\n",
    "                        subj_time[s]['t0']= el\n",
    "                    else:\n",
    "                        subj_time[s]['t1']= el\n",
    "            if (subj_time[s]['t1'] is not None) and (subj_time[s]['t0'] is not None):\n",
    "                X_temp = (X['all'][subj_time[s]['t1']]-X['all'][subj_time[s]['t0']]).reshape(1, X['all'].shape[1])\n",
    "                X['delta1'].append(X_temp)\n",
    "                subjects['delta1'].append(s)\n",
    "                Y['delta1'].append(Y['all'][indeces_temp[0]])\n",
    "            if (subj_time[s]['t2'] is not None) and (subj_time[s]['t1'] is not None):\n",
    "                X_temp = (X['all'][subj_time[s]['t2']]-X['all'][subj_time[s]['t1']]).reshape(1, X['all'].shape[1])\n",
    "                X['delta2'].append(X_temp)\n",
    "                subjects['delta2'].append(s)\n",
    "                Y['delta2'].append(Y['all'][indeces_temp[0]])\n",
    "            if (subj_time[s]['t2'] is not None and subj_time[s]['t0'] is not None):\n",
    "                X_temp = (X['all'][subj_time[s]['t2']]-X['all'][subj_time[s]['t0']]).reshape(1, X['all'].shape[1])\n",
    "                X['delta3'].append(X_temp)\n",
    "                subjects['delta3'].append(s)\n",
    "                Y['delta3'].append(Y['all'][indeces_temp[0]])\n",
    "        X['delta1'] = np.vstack( X['delta1'])\n",
    "        X['delta2'] = np.vstack( X['delta2'])\n",
    "        X['delta3'] = np.vstack( X['delta3'])\n",
    "        \n",
    "        loc = (file_name)\n",
    "\n",
    "        # To open Workbook\n",
    "        wb = xlrd.open_workbook(loc)\n",
    "        sheet = wb.sheet_by_index(0)\n",
    "\n",
    "        # For row 0 and column 0\n",
    "        IDs = np.array(sheet.col_values(3)[8:])\n",
    "        NetCalc = np.array(sheet.col_values(6)[8:])\n",
    "        frQ = np.array(sheet.col_values(9)[8:])\n",
    "        #print \"The matrix we will work with is: \", X[mode]\n",
    "         \n",
    "    return X[mode], np.array(Y[mode]), np.array(subjects[mode]), IDs, NetCalc, frQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read params from command line\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"-c\", dest=\"C\", default=1, type=float)\n",
    "parser.add_argument(\"-eps\", dest=\"epsilon\", default=0.1, type=float)\n",
    "parser.add_argument(\"-ker\", dest=\"kernel\", default='rbf')\n",
    "parser.add_argument(\"-gam\", dest=\"gamma\", default=0.0001, type=float)\n",
    "parser.add_argument(\"-deg\", dest=\"degree\", default=3, type=int)\n",
    "parser.add_argument(\"-fs\", dest=\"feature_selection\", default=None)\n",
    "parser.add_argument(\"-step\", dest=\"step\", default=0.3, type=float)\n",
    "parser.add_argument(\"-nfe\", dest=\"num_features\", default=150, type=int)\n",
    "parser.add_argument(\"-v\", dest=\"verbose\", default=False, type=bool)\n",
    "parser.add_argument(\"-m\", dest=\"method\", default='svm')\n",
    "parser.add_argument(\"-vt\", dest=\"verbose_train\", default=False, type=bool)\n",
    "parser.add_argument(\"-fn\", dest=\"file_name\", default='OGTT_INPUT.xlsx')\n",
    "parser.add_argument(\"-mtr\", dest=\"matrix\", default='between')\n",
    "parser.add_argument(\"-nc\", dest=\"num_comp\", default=1, type=int)\n",
    "parser.add_argument(\"-nt\", dest=\"num_trees\", default=200, type=int)\n",
    "parser.add_argument(\"-md\", dest=\"max_depth\", default=4, type=int)\n",
    "parser.add_argument(\"-mf\", dest=\"max_features\", default=1, type=int)\n",
    "parser.add_argument(\"-msl\", dest=\"min_samples_leaf\", default=1, type=int)\n",
    "parser.add_argument(\"-nr\", dest=\"num_repeats\", default=10, type=int)\n",
    "parser.add_argument(\"-np\", dest=\"num_permutations\", default=0, type=int)\n",
    "parser.add_argument(\"-nf\", dest=\"num_folds\", default=20, type=int)\n",
    "parser.add_argument(\"-sc\", dest=\"scaling\", default=True, type=bool)\n",
    "parser.add_argument(\"-ddf\", dest=\"ddof\", default=1, type=int)\n",
    "parser.add_argument(\"-mod\", dest=\"mode\", default='all')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "#run script with parameters\n",
    "results = []\n",
    "\n",
    "model_params = {}\n",
    "for mod in ['rf', 'svm', 'pls', 'lda', 'nn']:\n",
    "    model_params[mod] = {}\n",
    "    model_params[mod]['n_trees'] = None\n",
    "    model_params[mod]['max_depth'] = None\n",
    "    model_params[mod]['max_features'] = None\n",
    "    model_params[mod]['min_samples_leaf'] = None\n",
    "    model_params[mod]['C'] = None\n",
    "    model_params[mod]['epsilon'] = None\n",
    "    model_params[mod]['kernel'] = None\n",
    "    model_params[mod]['gamma'] = None\n",
    "    model_params[mod]['degree'] = None\n",
    "    model_params[mod]['n_comp'] = None\n",
    "\n",
    "model_params['rf']['n_trees'] = args.num_trees\n",
    "model_params['rf']['max_depth'] = args.max_depth \n",
    "model_params['rf']['max_features'] = args.max_features\n",
    "model_params['rf']['min_samples_leaf'] = args.min_samples_leaf\n",
    "model_params['svm']['C'] = args.C\n",
    "model_params['svm']['epsilon'] = args.epsilon\n",
    "model_params['svm']['kernel'] = args.kernel\n",
    "model_params['svm']['gamma'] = args.gamma\n",
    "model_params['svm']['degree'] = args.degree\n",
    "model_params['pls']['n_comp'] = args.num_comp\n",
    "model_params['lda']['n_comp'] = args.num_comp\n",
    "\n",
    "betas = {'all':[], 'delta1':[], 'delta2':[], 'delta3':[], 'within':[], 'between':[], 'ID':[]}\n",
    "modes = ['delta1', 'all', 'between', 'within', 'delta2', 'delta3']\n",
    "#modes = ['delta1']\n",
    "\n",
    "#num_components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "#epsilons = [0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "#c_values = [1, 10, 100, 1000, 10000, 100000]\n",
    "\n",
    "for m in modes:\n",
    "    if m=='all':\n",
    "        args.matrix = 'all'\n",
    "        args.mode = 'all'\n",
    "    elif m=='between':\n",
    "        args.matrix = 'between'\n",
    "        args.mode = 'all'\n",
    "    elif m=='within':\n",
    "        args.matrix = 'within'\n",
    "        args.mode = 'all'\n",
    "    else:\n",
    "        args.mode = m\n",
    "        args.matrix = 'all'\n",
    "    print \"Now the matrix is: \", args.matrix\n",
    "    print \"Now the mode is: \", args.mode\n",
    "    results_temp, coef,  IDs = full_script(args.num_folds,args.num_repeats,args.scaling, args.num_permutations, \n",
    "                                           args.matrix, args.file_name, args.verbose, args.ddof, args.method, \n",
    "                                           args.feature_selection, args.num_features, args.step, \n",
    "                                           model_params[args.method], args.verbose_train, args.mode)\n",
    "    results_temp['mode'] = m\n",
    "    print \"Results: \", results_temp\n",
    "    betas[m] = coef\n",
    "    sys.stdout.flush()\n",
    "    results.append(results_temp)\n",
    "\n",
    "betas_t = betas\n",
    "betas_t['ID']= IDs\n",
    "\n",
    "dest_file = \"coef_test2.csv\"\n",
    "with open(dest_file, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(betas_t.keys())\n",
    "    writer.writerows(zip(*betas_t.values()))\n",
    "\n",
    "res_file = \"res2_\"+args.method +\"_\"+str(args.num_features)+\"_\"+args.feature_selection+\"_\"+str(args.step)+\".csv\"\n",
    "\n",
    "with open(res_file, 'w') as csvfile:\n",
    "    fieldnames = ['crossval_auc', 'crossval_error', 'crossval_Q', 'crossval_train_auc', 'crossval_train_err',\n",
    "                  'method', 'feature_selection', 'num_features', 'step', 'n_trees', 'max_depth', 'max_features', \n",
    "                  'num_comp', 'C', 'epsilon', 'kernel', 'gamma', 'degree', 'mode']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for r in results:\n",
    "        results_t = { your_key: r[your_key] for your_key in fieldnames}\n",
    "        writer.writerow(results_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.42567568 -0.42567568 -0.42567568  0.57432432  0.57432432 -0.42567568\n",
      "  0.57432432  0.57432432  0.57432432  0.57432432  0.57432432 -0.42567568\n",
      "  0.57432432 -0.42567568  0.57432432 -0.42567568  0.57432432 -0.42567568\n",
      "  0.57432432  0.57432432  0.57432432 -0.42567568  0.57432432  0.57432432\n",
      " -0.42567568 -0.42567568 -0.42567568  0.57432432 -0.42567568  0.57432432\n",
      " -0.42567568 -0.42567568  0.57432432 -0.42567568 -0.42567568  0.57432432\n",
      " -0.42567568 -0.42567568 -0.42567568  0.57432432 -0.42567568 -0.42567568\n",
      " -0.42567568  0.57432432 -0.42567568 -0.42567568 -0.42567568 -0.42567568\n",
      " -0.42567568 -0.42567568 -0.42567568 -0.42567568  0.57432432  0.57432432\n",
      " -0.42567568  0.57432432  0.57432432  0.57432432  0.57432432  0.57432432\n",
      " -0.42567568  0.57432432 -0.42567568  0.57432432 -0.42567568  0.57432432\n",
      " -0.42567568  0.57432432  0.57432432  0.57432432 -0.42567568  0.57432432\n",
      " -0.42567568  0.57432432 -0.42567568 -0.42567568 -0.42567568  0.57432432\n",
      " -0.42567568  0.57432432 -0.42567568 -0.42567568  0.57432432 -0.42567568\n",
      " -0.42567568  0.57432432 -0.42567568 -0.42567568 -0.42567568  0.57432432\n",
      " -0.42567568 -0.42567568 -0.42567568  0.57432432 -0.42567568 -0.42567568\n",
      " -0.42567568 -0.42567568 -0.42567568 -0.42567568 -0.42567568  0.57432432\n",
      "  0.57432432 -0.42567568  0.57432432  0.57432432  0.57432432  0.57432432\n",
      "  0.57432432 -0.42567568  0.57432432 -0.42567568  0.57432432 -0.42567568\n",
      "  0.57432432 -0.42567568  0.57432432  0.57432432  0.57432432 -0.42567568\n",
      "  0.57432432 -0.42567568  0.57432432 -0.42567568 -0.42567568 -0.42567568\n",
      "  0.57432432 -0.42567568  0.57432432 -0.42567568 -0.42567568  0.57432432\n",
      " -0.42567568 -0.42567568  0.57432432 -0.42567568 -0.42567568 -0.42567568\n",
      "  0.57432432 -0.42567568 -0.42567568 -0.42567568  0.57432432 -0.42567568\n",
      " -0.42567568 -0.42567568 -0.42567568 -0.42567568]\n"
     ]
    }
   ],
   "source": [
    "print center_data(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.41869987 -0.4138034   0.55404984 ... -0.41989985 -0.37719956\n",
      "  -0.34189403]\n",
      " [-0.41869987 -0.4138034   0.30291055 ... -0.41989985  0.75707067\n",
      "   1.6584335 ]\n",
      " [-0.41869987  2.79511927  0.10165121 ... -0.41989985  0.2304199\n",
      "   1.76528413]\n",
      " ...\n",
      " [-0.41869987 -0.4138034   0.0638716  ... -0.41989985 -0.37719956\n",
      "   1.15579882]\n",
      " [-0.41869987 -0.4138034  -0.99350371 ... -0.41989985 -0.37719956\n",
      "  -0.34189403]\n",
      " [-0.41869987 -0.4138034  -0.99350371 ...  2.58170161 -0.37719956\n",
      "  -0.34189403]]\n"
     ]
    }
   ],
   "source": [
    "print scale_data(center_data(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.85800263 -0.85800263 -0.85800263  1.1576226   1.1576226  -0.85800263\n",
      "  1.1576226   1.1576226   1.1576226   1.1576226   1.1576226  -0.85800263\n",
      "  1.1576226  -0.85800263  1.1576226  -0.85800263  1.1576226  -0.85800263\n",
      "  1.1576226   1.1576226   1.1576226  -0.85800263  1.1576226   1.1576226\n",
      " -0.85800263 -0.85800263 -0.85800263  1.1576226  -0.85800263  1.1576226\n",
      " -0.85800263 -0.85800263  1.1576226  -0.85800263 -0.85800263  1.1576226\n",
      " -0.85800263 -0.85800263 -0.85800263  1.1576226  -0.85800263 -0.85800263\n",
      " -0.85800263  1.1576226  -0.85800263 -0.85800263 -0.85800263 -0.85800263\n",
      " -0.85800263 -0.85800263 -0.85800263 -0.85800263  1.1576226   1.1576226\n",
      " -0.85800263  1.1576226   1.1576226   1.1576226   1.1576226   1.1576226\n",
      " -0.85800263  1.1576226  -0.85800263  1.1576226  -0.85800263  1.1576226\n",
      " -0.85800263  1.1576226   1.1576226   1.1576226  -0.85800263  1.1576226\n",
      " -0.85800263  1.1576226  -0.85800263 -0.85800263 -0.85800263  1.1576226\n",
      " -0.85800263  1.1576226  -0.85800263 -0.85800263  1.1576226  -0.85800263\n",
      " -0.85800263  1.1576226  -0.85800263 -0.85800263 -0.85800263  1.1576226\n",
      " -0.85800263 -0.85800263 -0.85800263  1.1576226  -0.85800263 -0.85800263\n",
      " -0.85800263 -0.85800263 -0.85800263 -0.85800263 -0.85800263  1.1576226\n",
      "  1.1576226  -0.85800263  1.1576226   1.1576226   1.1576226   1.1576226\n",
      "  1.1576226  -0.85800263  1.1576226  -0.85800263  1.1576226  -0.85800263\n",
      "  1.1576226  -0.85800263  1.1576226   1.1576226   1.1576226  -0.85800263\n",
      "  1.1576226  -0.85800263  1.1576226  -0.85800263 -0.85800263 -0.85800263\n",
      "  1.1576226  -0.85800263  1.1576226  -0.85800263 -0.85800263  1.1576226\n",
      " -0.85800263 -0.85800263  1.1576226  -0.85800263 -0.85800263 -0.85800263\n",
      "  1.1576226  -0.85800263 -0.85800263 -0.85800263  1.1576226  -0.85800263\n",
      " -0.85800263 -0.85800263 -0.85800263 -0.85800263]\n"
     ]
    }
   ],
   "source": [
    "print scale_data(center_data(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb, Xw = split_between_within_subject_matrix(center_data(X), subj, np.unique(subj), len(np.unique(subj)), len(subj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1024627.53378378,  -261774.16216216,   687161.50225225, ...,\n",
       "          357393.29054054, -2752227.0472973 ,   155966.9481982 ],\n",
       "       [-1024627.53378378,  -261774.16216216,  -245302.16441441, ...,\n",
       "          374305.95720721,     6497.61936937,  1035459.28153153],\n",
       "       [ -441580.20045045,   414886.17117117,   186630.83558559, ...,\n",
       "        -1030849.70945946, -1274400.38063063,  1148976.9481982 ],\n",
       "       ...,\n",
       "       [-1024627.53378378,  -261774.16216216,   280070.16891892, ...,\n",
       "        -1030849.70945946, -2752227.0472973 ,  3531994.11486486],\n",
       "       [-1024627.53378378,  -261774.16216216, -1827586.83108108, ...,\n",
       "        -1030849.70945946, -2752227.0472973 , -1089680.38513514],\n",
       "       [-1024627.53378378,   301326.5045045 ,  -653848.16441441, ...,\n",
       "         1425450.29054054, -2752227.0472973 , -1089680.38513514]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  3.32033667e+05, ...,\n",
       "        -1.38824300e+06,  0.00000000e+00, -1.24564733e+06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  8.02517333e+05, ...,\n",
       "        -1.40515567e+06,  5.51744933e+06,  4.25027933e+06],\n",
       "       [-5.83047333e+05,  1.35332067e+06,  3.60333333e+02, ...,\n",
       "         0.00000000e+00,  2.95565333e+06,  4.47731467e+06],\n",
       "       ...,\n",
       "       [ 0.00000000e+00,  0.00000000e+00, -1.62576000e+05, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.51753500e+05],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -5.63100667e+05, -1.17373867e+06, ...,\n",
       "         4.91260000e+06,  0.00000000e+00,  0.00000000e+00]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  2.19098707e-01 ... -6.95248637e-01\n",
      "   0.00000000e+00 -4.72571311e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  5.29556271e-01 ... -7.03718702e-01\n",
      "   1.09557080e+00  1.61246287e+00]\n",
      " [-2.92301347e-01  2.67035475e+00  2.37772779e-04 ...  0.00000000e+00\n",
      "   5.86888485e-01  1.69859510e+00]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00 -1.07278855e-01 ...  0.00000000e+00\n",
      "   0.00000000e+00  5.75719535e-02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00 -1.11110292e+00 -7.74513702e-01 ...  2.46028862e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print scale_data(Xw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_simple_classifier(X, Y, subj, num_samples, mean,deviation, ddof, method, feature_selection, model_params,\n",
    "               num_features, step, verbose_train, scaling):\n",
    "    if scaling:\n",
    "        Y_temp = np.array(Y)\n",
    "        Y_temp = descale_data(matrix=Y_temp, deviation=deviation, ddof=ddof) #descale\n",
    "        Y_temp = Y_temp+mean                             #add mean\n",
    "        for j in range(len(Y_temp)):\n",
    "            if Y_temp[j]>1:\n",
    "                Y_temp[j]=1 \n",
    "            elif Y_temp[j]<0:\n",
    "                Y_temp[j]=0\n",
    "                Y_temp=Y_temp.astype(int)\n",
    "    else:\n",
    "        Y_temp = Y+mean\n",
    "    model = PLSRegression(n_components=model_params['n_comp'], scale=False) #initialize a generic PLS model\n",
    "    model_temp = copy.copy(model)\n",
    "    model_temp.fit(X, Y)\n",
    "    print \"coeffs: \", model_temp.coef_\n",
    "    print \"coeffs: \", np.shape(model_temp.coef_)\n",
    "    print \"new number: \", np.rint(X.shape[1]/2)\n",
    "    print \"type of new number: \", type(np.rint(X.shape[1]/2))\n",
    "    \n",
    "    features = np.range(X.shape[1])\n",
    "    #features = np.argpartition(model_temp.coef_.reshape(np.shape(model_temp.coef_)[0],), int(np.rint(X.shape[1]/2)))\n",
    "\n",
    "    #X_new = X[:, features]\n",
    "    if X.shape[1]>num_features:\n",
    "        features = np.argpartition(model_temp.coef_.reshape(np.shape(model_temp.coef_)[0],),int(np.rint(X.shape[1]/2)))\n",
    "        X_new = X[:, features]\n",
    "        new_features, model, train_error, auc, coeff = recursive_simple_classifier(X_new, Y, subj, num_samples, mean,\n",
    "                                                                                   deviation, ddof, method, \n",
    "                                                                                   feature_selection, model_params,\n",
    "                                                                                   num_features, step, verbose_train, \n",
    "                                                                                   scaling)\n",
    "        X_new = X_new[:, new_features]\n",
    "    else:\n",
    "        features = np.range(X.shape[1])\n",
    "        X_new = copy.copy(X)\n",
    "    model.fit(X_new, Y) #ACTUALLY getting the classifier model, fit model to data\n",
    "    pred = model.predict(X_new) #predict the values on the train set\n",
    "    pred_temp = pred.flatten()\n",
    "    if scaling:\n",
    "        pred_temp = descale_data(matrix=pred_temp, deviation=deviation, ddof=ddof ) #descale\n",
    "        pred_temp = pred_temp+mean #add mean\n",
    "        pred_temp = round_num(pred_temp)\n",
    "    pred_temp = pred_temp+mean\n",
    "    train_error = mean_squared_error(y_true=Y_temp, y_pred=pred_temp)\n",
    "    fpr, tpr, auc = get_roc_auc(labels=Y_temp, predictions=pred_temp)#AUC if needed\n",
    "    return features, model, train_error, auc, coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 7 5 9]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 12, 13, 10, 1, 14]\n",
    "print np.argpartition(a, 4)[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
