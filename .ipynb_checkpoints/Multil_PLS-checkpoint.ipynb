{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.cross_decomposition import PLSCanonical\n",
    "import random\n",
    "import pandas as pd\n",
    "from sys import exit\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import cPickle as pickle\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "# USUAL Classifier    \n",
    "def simple_classifier(X, Y, num_samples, mean,deviation, ddof, method, feature_selection, model_params,\n",
    "               num_features, step, verbose_train):\n",
    "    model = PLSRegression(n_components=model_params['n_comp'], scale=False) \n",
    "    model_temp = copy.copy(model)\n",
    "    model_temp.fit(X, Y)\n",
    "    rfe = RFE(estimator=model, n_features_to_select=num_features, step=step)\n",
    "    fit = rfe.fit(X, Y)\n",
    "    features = fit.support_\n",
    "    X_new = X[:, features]\n",
    "    model.fit(X_new, Y) #ACTUALLY getting the classifier model, fit model to data\n",
    "    pred = model.predict(X_new)\n",
    "    coeff = model.coef_\n",
    "    diff = Y - pred\n",
    "    train_error = sum(diff*diff) #calculate square error\n",
    "    train_error_temp = mean_squared_error(y_true=Y, y_pred=pred)\n",
    "    #fpr, tpr, auc = get_roc_auc(labels=Y_temp, predictions=pred_temp)\n",
    "    auc = 1.0\n",
    "    return features, model, train_error, auc, coeff\n",
    "    \n",
    "def classifier(X, Y, num_samples, mean,deviation, ddof, method, feature_selection, model_params,\n",
    "               num_features, step, verbose_train):\n",
    "    \n",
    "    #choose, which classifier to use\n",
    "    if method== 'rf':#build a generic Random Forest\n",
    "        model = RandomForestRegressor(n_jobs=12, n_estimators=model_params['n_trees'], random_state=0, \n",
    "                                      max_features=model_params['max_features'], max_depth=model_params['max_depth'], \n",
    "                                      min_samples_leaf=model_params['min_samples_leaf'])\n",
    "    if method == 'pls':\n",
    "        model = PLSRegression(n_components=model_params['n_comp'], scale=False) #initialize a generic PLS model with parameters\n",
    "        \n",
    "    if method == 'svm':\n",
    "        model = SVR(C= model_params['C'], epsilon=model_params['epsilon'], kernel=model_params['kernel'], \n",
    "                   gamma = model_params['gamma'], degree=model_params['degree'])\n",
    "        \n",
    "    #choose a method for feature selection\n",
    "    if feature_selection == 'FromModel':\n",
    "        model_temp = model\n",
    "        model_temp.fit(X, Y)\n",
    "        model_temp = SelectFromModel(model, prefit=True)\n",
    "        features = model_temp.get_support()\n",
    "    elif feature_selection == 'rfecv':\n",
    "        rfe = RFECV(n_jobs=12, estimator=model, cv=20, step=step)\n",
    "        fit = rfe.fit(X, Y)\n",
    "        features = fit.support_\n",
    "    elif feature_selection == 'rfe':\n",
    "        model_temp = model\n",
    "        model_temp.fit(X, Y)\n",
    "        rfe = RFE(estimator=model, n_features_to_select=num_features, step=step)\n",
    "        fit = rfe.fit(X, Y)\n",
    "        features = fit.support_\n",
    "    elif feature_selection is None:\n",
    "        features = [True for f in range(X.shape[1])]\n",
    "    X_new = X[:, features]\n",
    "    model.fit(X_new, Y) #ACTUALLY getting the classifier model, fit model to data\n",
    "    pred = model.predict(X_new) #predict the values on the train set\n",
    "    if method == 'rf':\n",
    "        coeff = model.feature_importances_.flatten()\n",
    "    elif method == 'pls' or method == 'svm':\n",
    "        coeff = model.coef_.flatten()\n",
    "    #process the predictions\n",
    "    pred_temp = pred.flatten()\n",
    "    pred_temp = descale_data(matrix=pred_temp, deviation=deviation, ddof=ddof ) #descale\n",
    "    pred_temp = pred_temp+mean #add mean\n",
    "    \n",
    "    #process the labels\n",
    "    Y_temp = np.array(Y)\n",
    "    Y_temp = descale_data(matrix=Y_temp, deviation=deviation, ddof=ddof) #descale\n",
    "    Y_temp = Y_temp+mean                             #add mean\n",
    "    #for j in range(len(Y_temp)):\n",
    "    #    if Y_temp[j]>1:\n",
    "    #        Y_temp[j]=1 \n",
    "    #    elif Y_temp[j]<0:\n",
    "    #         Y_temp[j]=0\n",
    "    #Y_temp=Y_temp.astype(int)\n",
    "    \n",
    "    pred_temp = np.rint(pred_temp)\n",
    "    \n",
    "    diff = Y_temp - pred_temp\n",
    "    train_error = sum(diff*diff) #calculate square error\n",
    "    train_error_temp = mean_squared_error(y_true=Y_temp, y_pred=pred_temp)\n",
    "    fpr, tpr, auc = get_roc_auc(labels=Y_temp, predictions=pred_temp)#AUC if needed\n",
    "    \n",
    "    if verbose_train:\n",
    "        print \"Training error: \", train_error/num_samples\n",
    "        print \"Training error computed in library: \", train_error_temp\n",
    "        print \"Training auc: \", auc\n",
    "        plot_roc_curve(fpr, tpr, auc)\n",
    "    \n",
    "    return features, model, train_error, auc, coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_data(matrix, mean=None):\n",
    "    if mean is None:\n",
    "        matrix = matrix - np.mean(a=matrix, axis=0)\n",
    "    else:\n",
    "        matrix = matrix - mean\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(matrix, deviation, ddof):\n",
    "    if deviation is None:\n",
    "        deviation= np.std(a=matrix, axis=0, ddof=ddof)\n",
    "    matrix_temp = matrix.copy()\n",
    "    matrix = matrix/deviation\n",
    "    if np.isnan(matrix).any():\n",
    "        matrix = matrix_temp\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descale_data(matrix, deviation, ddof):\n",
    "    if deviation is None:\n",
    "        matrix = matrix*np.std(a=matrix, axis=0, ddof=ddof)\n",
    "    else:\n",
    "        matrix = matrix*deviation\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_auc(labels, predictions):\n",
    "    #compute precision-recall curve\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true=labels, y_score=predictions, drop_intermediate=False) \n",
    "    #compute area under the curve for this run\n",
    "    auc = metrics.roc_auc_score(y_true=labels, y_score=predictions, average='macro', sample_weight=None)\n",
    "    return fpr, tpr, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, auc):\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MULTILEVEL PLS\n",
    "def perform_multilevel_pls(X, Y, subj, unique_subj, num_unique_subj, num_subj, scaling, par, ddof, method, \n",
    "                           feature_selection, num_features, step, model_params, verbose_train):\n",
    "    \n",
    "    #mean-centering\n",
    "    X_centered = center_data(matrix=X, mean=None) #mean centering of data\n",
    "    Y_centered = center_data(matrix=Y, mean=None) \n",
    "\n",
    "    #split matrix into between and within subject variations\n",
    "    if par!='all':\n",
    "        Xb, Xw = split_between_within_subject_matrix(X=X_centered, subj=subj, unique_subj=unique_subj, \n",
    "                                                 num_unique_subj=num_unique_subj, num_subj=num_subj)\n",
    "    else:\n",
    "        Xb = None\n",
    "        Xw = None\n",
    "    \n",
    "    #scaling (if necessary and specified)\n",
    "    if scaling: #scale the data, if the flag is true\n",
    "        X_scaled = scale_data(matrix=X_centered, deviation=None, ddof=ddof)\n",
    "        Y_scaled = scale_data(matrix=Y_centered, deviation=None, ddof=ddof)                          \n",
    "        if par!= 'all':\n",
    "            Xb_scaled = scale_data(matrix=Xb, deviation=None, ddof=ddof)\n",
    "            Xw_scaled = scale_data(matrix=Xw, deviation=None, ddof=ddof)\n",
    "    else:\n",
    "        X_scaled = X_centered\n",
    "        Y_scaled = Y_centered\n",
    "        if par!= 'all':\n",
    "            Xb_scaled = Xb\n",
    "            Xw_scaled = Xw\n",
    "    \n",
    "    #which matrix are we interested in\n",
    "    if par=='all':\n",
    "        X_target = X_scaled\n",
    "    elif par=='between':\n",
    "        X_target = Xb_scaled\n",
    "    elif par=='within':\n",
    "        X_target = Xw_scaled\n",
    "    \n",
    "    start = time.time()\n",
    "    #perform classifier (PLS or other) on target data\n",
    "    features, model, train_error, train_auc, coeff = simple_classifier(X=X_target, Y=Y_scaled, num_samples=num_subj,\n",
    "                                                           mean=np.mean(Y), deviation= np.std(Y, axis=0, ddof=ddof), \n",
    "                                                           ddof=ddof, method=method, \n",
    "                                                           feature_selection=feature_selection, \n",
    "                                                           num_features=num_features, step=step, \n",
    "                                                           model_params=model_params, verbose_train=verbose_train)\n",
    "    end = time.time()\n",
    "    print \"Time elapsed for simple classifier: \", (end - start)\n",
    "    return features, model, train_error, train_auc, Xb, Xw, Y_scaled, X_scaled, coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_train_test(X, Y, subj, train_subj, test_subj, num_subj):\n",
    "    \n",
    "    X_train = None #initialization\n",
    "    X_test = None\n",
    "    Y_train = []\n",
    "    Y_test = []\n",
    "    subj_train = []\n",
    "    subj_test = []  \n",
    "    \n",
    "    #create the test and train dataset from matrix X with chosen subjects\n",
    "    mask = np.isin(subj,train_subj)\n",
    "    inverse_mask = np.invert(mask)\n",
    "\n",
    "    subj_train = subj[mask]\n",
    "    subj_test = subj[inverse_mask]\n",
    "    X_train = X[mask]\n",
    "    Y_train = Y[mask]\n",
    "    X_test = X[inverse_mask]\n",
    "    Y_test = Y[inverse_mask]\n",
    "\n",
    "    num_train_subj = len(subj_train)          #how many entries in train dataset\n",
    "    num_test_subj= num_subj - num_train_subj  #how many entries in test dataset\n",
    "        \n",
    "    return X_train, X_test, Y_train, Y_test, subj_train, subj_test, num_train_subj, num_test_subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL SCRIPT\n",
    "def full_script(num_folds, num_repeats, scaling, num_permutations, par, filename, verbose, ddof, method, \n",
    "                feature_selection,num_features,step, model_params,verbose_train, mean):\n",
    "    \n",
    "    #READ OR MAKE UP DATA\n",
    "    X, Y, subj, IDs, NetCalc, frQ = read_data(filename, mean)\n",
    "    \n",
    "    #create list of unique subjects\n",
    "    unique_subj = np.unique(subj) \n",
    "    #the number of entries = number of subjects corresponding to entries (1 subject per entry)\n",
    "    num_subj = len(subj)    \n",
    "    #the number of unique subjects\n",
    "    num_unique_subj = len(unique_subj) \n",
    "    \n",
    "    \n",
    "    #initialization of error\n",
    "    error = 0 \n",
    "    permutation_error=None\n",
    "    permutation_auc=None \n",
    "    permutation_Q=None\n",
    "    crossval_error=None\n",
    "    crossval_auc=None\n",
    "    crossval_Q=None\n",
    "    full_train_error=None\n",
    "    full_train_auc=None\n",
    "    crossval_error=None\n",
    "    crossval_auc=None\n",
    "    crossval_Q=None\n",
    "    crossval_train_err=None\n",
    "    crossval_train_auc=None\n",
    "    \n",
    "\n",
    "    #PERFORMING MULTILEVEL PLS ON WHOLE DATASET\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    features, full_model, full_train_error, full_train_auc, Xb, Xw, Y_scaled, X_scaled, coeff = \\\n",
    "    perform_multilevel_pls(X=X, Y=Y, subj=subj, unique_subj=unique_subj, num_subj=num_subj, \n",
    "                           num_unique_subj=num_unique_subj, scaling=scaling, par=par, \n",
    "                           ddof=ddof, method=method, feature_selection=feature_selection, \n",
    "                           num_features=num_features, step=step, model_params=model_params, verbose_train=verbose_train)\n",
    "    end = time.time()\n",
    "    print \"Time elapsed for whole multilevel: \", (end - start)\n",
    "    #print \"Features chosen: \", IDs[features]\n",
    "    #print \"Biomarkers chosen: \", NetCalc[features]\n",
    "    #print \"rfQ chosen: \", frQ[features]\n",
    "    #print \"Coefficients: \", coeff\n",
    "    \n",
    "    if verbose: \n",
    "        print \"CROSS_VALIDATION ON ACTUAL DATA: \"\n",
    "        \n",
    "    #CROSS-VALIDATION\n",
    "    crossval_error, crossval_auc, crossval_Q, crossval_train_err, crossval_train_auc =\\\n",
    "    cross_validation(X=X, Y=Y, subj=subj, unique_subj=unique_subj, num_subj=num_subj, num_unique_subj=num_unique_subj, \n",
    "                     num_folds=num_folds, num_repeats=num_repeats, scaling=scaling, par=par, \n",
    "                     verbose=verbose, ddof=ddof, method=method, feature_selection=feature_selection, \n",
    "                     num_features = num_features, step=step, model_params=model_params, verbose_train=verbose_train)\n",
    "    \n",
    "    if verbose: \n",
    "        print \"PERMUTATED DATA CROSS_VALIDATION: \"\n",
    "        \n",
    "    #PERMUTATE\n",
    "    permutation_error, permutation_auc, permutation_Q, permutation_train_error, permutation_train_auc =\\\n",
    "                                                        validate_permutation(X=X, Y=Y, subj=subj,\n",
    "                                                                             unique_subj=unique_subj, \n",
    "                                                                             num_subj=num_subj,\n",
    "                                                                             num_unique_subj=num_unique_subj,\n",
    "                                                                             num_folds=num_folds,\n",
    "                                                                             num_repeats=num_repeats, \n",
    "                                                                             scaling=scaling,\n",
    "                                                                             num_permutations=num_permutations, \n",
    "                                                                             par=par, verbose=verbose, \n",
    "                                                                             ddof=ddof,method=method,\n",
    "                                                                             feature_selection=feature_selection, \n",
    "                                                                             num_features=num_features, step=step,\n",
    "                                                                             model_params=model_params, \n",
    "                                                                             verbose_train=verbose_train)\n",
    "    \n",
    "    results = {'num_folds':num_folds,'num_repeats':num_repeats,'scaling':scaling,'num_permutations':num_permutations,\n",
    "               'par':par, 'filename':filename, 'verbose':verbose, 'ddof':ddof, 'method':method, \n",
    "               'feature_selection':feature_selection, 'num_features':num_features, 'step':step, \n",
    "               'n_trees':model_params['n_trees'], 'max_depth':model_params['max_depth'], \n",
    "               'max_features':model_params['max_features'], 'min_samples_leaf':model_params['min_samples_leaf'],\n",
    "               'num_comp':model_params['n_comp'], 'full_train_error':full_train_error, 'full_train_auc':full_train_auc, \n",
    "               'crossval_error':crossval_error, 'crossval_auc':crossval_auc, 'crossval_Q':crossval_Q, \n",
    "               'crossval_train_err': crossval_train_err, 'crossval_train_auc':crossval_train_auc, \n",
    "               'permutation_error': permutation_error, 'permutation_auc':permutation_auc, 'permutation_Q':permutation_Q, \n",
    "               'C': model_params['C'], 'gamma': model_params['gamma'], 'epsilon': model_params['epsilon'], \n",
    "               'degree': model_params['degree'], 'kernel': model_params['kernel'] }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CROSS-VALIDATION\n",
    "def cross_validation(X, Y, subj, unique_subj, num_subj, num_unique_subj, num_folds, num_repeats, scaling, par, \n",
    "                     verbose, ddof, method, feature_selection, num_features, step, model_params,\n",
    "                     verbose_train):\n",
    "    #initialization\n",
    "    error       = 0 \n",
    "    auc         = 0\n",
    "    train_error = 0\n",
    "    train_auc   = 0\n",
    "    \n",
    "    #create array of labels for each subject\n",
    "    Y_temp = [Y[np.where(subj==unique_subj[i])[0][0]] for i in range(num_unique_subj)]\n",
    "    Y=np.array(Y)\n",
    "    \n",
    "    #repeat cross_validation as many times as specified\n",
    "    for i in range(num_repeats): \n",
    "        kf = StratifiedKFold(n_splits=num_folds) #KFold cross validation that keeps the ratio between classes\n",
    "        kf.get_n_splits(X, Y)\n",
    "        \n",
    "        for train_index, test_index in kf.split(X[:num_unique_subj], Y_temp):#repeat with every fold as test set\n",
    "            train_subj = unique_subj[train_index]\n",
    "            test_subj = unique_subj[test_index]\n",
    "        \n",
    "            X_train, X_test, Y_train, Y_test, subj_train, subj_test, num_train_subj, num_test_subj = \\\n",
    "                                                    separate_train_test(X=X, Y=Y, subj=subj, train_subj=train_subj,\n",
    "                                                                        test_subj=test_subj, num_subj=num_subj)\n",
    "                        \n",
    "            num_unique_train_subj=len(train_subj)\n",
    "            num_unique_test_subj=len(test_subj)\n",
    "            \n",
    "            X_train_mean = np.mean(X_train, axis=0)\n",
    "            Y_train_mean = np.mean(Y_train, axis=0)\n",
    "            \n",
    "            features, model, train_error_temp, train_auc_temp, Xb_train, Xw_train, Y_scaled_train, X_scaled_train, coeff = \\\n",
    "                                       perform_multilevel_pls(X=X_train, Y=Y_train, subj=subj_train, \n",
    "                                                              unique_subj=train_subj, \n",
    "                                                              num_unique_subj=num_unique_train_subj, \n",
    "                                                              num_subj=num_train_subj, scaling=scaling, par=par, \n",
    "                                                              ddof=ddof, method=method, \n",
    "                                                              feature_selection=feature_selection, \n",
    "                                                              num_features=num_features, step=step,\n",
    "                                                              model_params=model_params, \n",
    "                                                              verbose_train=verbose_train)\n",
    "\n",
    "            X_centered_test = center_data(X_test, X_train_mean) #mean centering of data\n",
    "            Y_centered_test = center_data(Y_test, Y_train_mean)   \n",
    "\n",
    "            #split test data into between and within subject variation\n",
    "            if par!= 'all':\n",
    "                Xb_test, Xw_test = split_between_within_subject_matrix(X=X_centered_test, subj=subj_test, \n",
    "                                                                   unique_subj=test_subj,\n",
    "                                                                   num_unique_subj=num_unique_test_subj, \n",
    "                                                                   num_subj=num_test_subj )\n",
    "\n",
    "                \n",
    "            if scaling: #scale the data, if the flag is true\n",
    "                X_train_deviation = np.std(X_train, axis = 0, ddof=ddof)\n",
    "                Y_train_deviation = np.std(Y_train, axis = 0, ddof=ddof)\n",
    "                if par!= 'all':\n",
    "                    Xb_train_deviation = np.std(Xb_train, axis = 0, ddof=ddof)\n",
    "                    Xw_train_deviation = np.std(Xw_train, axis = 0, ddof=ddof)\n",
    "\n",
    "                X_scaled_test = scale_data(matrix=X_centered_test, deviation=X_train_deviation, ddof=ddof)\n",
    "                Y_scaled_test = scale_data(matrix=Y_centered_test, deviation=Y_train_deviation, ddof=ddof)\n",
    "                if par!= 'all':\n",
    "                    Xb_scaled_test = scale_data(matrix=Xb_test, deviation=Xb_train_deviation, ddof=ddof)\n",
    "                    Xw_scaled_test = scale_data(matrix=Xw_test, deviation=Xw_train_deviation, ddof=ddof)\n",
    "            else:\n",
    "\n",
    "                X_scaled_test = X_centered_test\n",
    "                Y_scaled_test = Y_centered_test\n",
    "                if par!= 'all':\n",
    "                    Xb_scaled_test = Xb_test\n",
    "                    Xw_scaled_test = Xw_test\n",
    "\n",
    "            #if only between or only within subject variations chosen, predict on that part of the matrix\n",
    "            if par=='all':\n",
    "                X_target = X_scaled_test\n",
    "            elif par=='between':\n",
    "                X_target = Xb_scaled_test\n",
    "            elif par=='within':\n",
    "                X_target = Xw_scaled_test\n",
    "            X_target = X_target[:, features]\n",
    "                     \n",
    "            pred = model.predict(X_target) #predict test data with model trained on the training set\n",
    "             \n",
    "            #process the predicted values\n",
    "            pred=pred.flatten()\n",
    "            pred= pred.reshape(np.product(pred.shape),)\n",
    "            pred = descale_data(matrix=pred, deviation=Y_train_deviation, ddof=ddof)\n",
    "            pred = pred+Y_train_mean\n",
    "\n",
    "            #compute the cross-validation cumulative error (squeared error)\n",
    "            diff = np.array(Y_test - pred) \n",
    "            err_temp= sum(diff*diff)\n",
    "\n",
    "            #get and plot ROC curve, if want\n",
    "            fpr, tpr, auc_temp = get_roc_auc(labels=Y_test, predictions=pred)        \n",
    "            if i%10==0 and verbose:\n",
    "                plot_roc_curve(fpr=fpr, tpr=tpr, auc=auc_temp)\n",
    "                \n",
    "            auc = auc + auc_temp            \n",
    "            error = error + err_temp #compute square error\n",
    "            train_error =train_error+train_error_temp\n",
    "            train_auc = train_auc+train_auc_temp\n",
    "    \n",
    "        \n",
    "    error = float(error)/(num_repeats*num_subj) #mean error for cross-validation\n",
    "    auc = auc/(num_repeats*num_folds)     #mean AUC score for cross validation\n",
    "    train_error = float(train_error)/(num_repeats*num_subj)\n",
    "    train_auc = float(train_auc)/(num_repeats*num_folds) \n",
    "    Q = 1 - error/(sum(Y*Y)/float(num_subj))\n",
    "    if verbose:\n",
    "        print \"Mean cross-validation error: \", error\n",
    "        print \"Mean cross-validation AUC: \", auc\n",
    "        print \"Cross-validation Q: \", Q\n",
    "    if verbose_train:\n",
    "        print \"Mean train cross-validation error: \", error\n",
    "        print \"Mean train cross-validation AUC: \", auc\n",
    "        \n",
    "    return error, auc, Q, train_error, train_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data matrix nito between and within subject variations\n",
    "def split_between_within_subject_matrix(X, subj, unique_subj, num_unique_subj, num_subj): #SPLIT MATRIX \n",
    "    Xb = np.zeros(X.shape) #initialization\n",
    "    Xw = np.zeros(X.shape)\n",
    "    means = np.zeros((num_unique_subj, X.shape[1]))\n",
    "    for j in range(num_unique_subj): #go through all unique subjects\n",
    "        #find indexes of all entries for a certain subject (unique_subj[j])\n",
    "        idx = np.where(np.array(subj)==unique_subj[j]) \n",
    "        #calculate mean for each subject unique_subj[j]      \n",
    "        means[j] = np.mean(X[idx[0]], axis=0)     \n",
    "    for i in range(num_subj): #go through subjects of all entries\n",
    "        #find the index of the subject corresponding to subj[i] in unique_subj\n",
    "        k = np.where(unique_subj==subj[i])\n",
    "        #create a matrix where all entries for subject = mean (between subject variation) \n",
    "        Xb[i] = means[k[0][0]]                  \n",
    "    Xw = X - Xb #get the within subjects matrix             \n",
    "    return Xb, Xw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMUTATION\n",
    "def validate_permutation(X, Y, subj, unique_subj, num_subj, num_unique_subj, num_folds, num_repeats, scaling, \n",
    "                         num_permutations, par, verbose, ddof, method, feature_selection, num_features, \n",
    "                         step, model_params, verbose_train):\n",
    "    \n",
    "    err       = [] #initialization\n",
    "    auc       = []\n",
    "    Q         = []\n",
    "    train_err = []\n",
    "    train_auc = []\n",
    "    \n",
    "    for i in range(num_permutations):#perform permutations as many times as specified\n",
    "        Y_temp = Y.copy()  #create temporary labels which then shuffle/permutate, so original is left untouched\n",
    "        \n",
    "        np.random.shuffle(Y_temp) #randomly shuffle the Y(labels) vector. Checks if your results will be similar\n",
    "        \n",
    "        #perform cross-validation for each permutation and get an array of accuracies when permutated\n",
    "        err_temp, auc_temp, Q_temp, train_error_temp, train_auc_temp  = cross_validation(X=X, Y=Y_temp, subj=subj, \n",
    "                                                                            unique_subj=unique_subj,\n",
    "                                                                            num_subj=num_subj, \n",
    "                                                                            num_unique_subj=num_unique_subj, \n",
    "                                                                            num_folds=num_folds, \n",
    "                                                                            num_repeats=num_repeats, \n",
    "                                                                            scaling=scaling, \n",
    "                                                                            par=par, \n",
    "                                                                            verbose=verbose,ddof=ddof,method=method, \n",
    "                                                                            feature_selection=feature_selection, \n",
    "                                                                            num_features=num_features, step=step, \n",
    "                                                                            model_params=model_params, \n",
    "                                                                            verbose_train=verbose_train)\n",
    "        \n",
    "        #save all the error and metrics values\n",
    "        err.append(err_temp)\n",
    "        auc.append(auc_temp)\n",
    "        Q.append(Q_temp)\n",
    "        train_err.append(train_err_temp)\n",
    "        train_auc.append(train_auc_temp)\n",
    "        \n",
    "    if len(err) >0 and verbose :\n",
    "        print \"Mean permutated squared error : \", np.mean(err)\n",
    "        print \"Mean permutated auc : \", np.mean(auc)\n",
    "        print \"Mean permutated Q: \", np.mean(Q)\n",
    "    elif verbose:\n",
    "        print \"No permutations performed\"\n",
    "    return err, auc, Q, train_err, train_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name=None, mode='delta1'):\n",
    "    if file_name is None: #if no input file specified, make up data\n",
    "        num_subj = 30\n",
    "        num_feat = 1000\n",
    "        X_out = np.random.rand(num_subj, num_feat)\n",
    "        for i in range(3*num_subj):\n",
    "            if i%2==0:\n",
    "                X[i, 0] = X[i, 0]+(np.random.rand()+0.75)*15\n",
    "                X[i, 3] = X[i, 3]+(np.random.rand()+0.75)*20\n",
    "                X[i, 6] = X[i, 6]+(np.random.rand()+0.75)*17\n",
    "        Y =    [1 if i%2 == 0 else 0 for i in range(3*num_subj)]\n",
    "        subjects = [1+i%num_subj for i in range(3*num_subj)]\n",
    "\n",
    "    else:\n",
    "        data = pd.read_excel(file_name) #read the excel file\n",
    "        #take relevant values as subject id's, convert to int\n",
    "        subjects = data.head().iloc[-2].values[0:148].astype(np.int64) \n",
    "        #take relevant values as labels, convert to int\n",
    "        \n",
    "        X = {}\n",
    "        subjects = {}\n",
    "        Y = {}\n",
    "        \n",
    "        #take the matabolite matrix, transpose, convert to int\n",
    "        X['all'] = data.values[7:, :-9].transpose().astype(np.int64)  \n",
    "        Y['all'] = (data.head().iloc[-3].values[0:148]=='risk').astype(np.int64)\n",
    "        subjects['all'] = data.head().iloc[-2].values[0:148].astype(np.int64)\n",
    "        \n",
    "        unique_subj = np.unique(subjects['all']) \n",
    "        num_unique_subj = len(unique_subj)\n",
    "        num_subj = len(subjects['all'])\n",
    "        subject_time = {}\n",
    "        mask_temp = np.array([subjects['all'][index+1]-subjects['all'][index] for index in range(num_subj-1)])\n",
    "        lines = np.where(mask_temp<0)[0]\n",
    "        X['delta1'] = []\n",
    "        X['delta2'] = []\n",
    "        X['delta3'] = []\n",
    "        subjects['delta1'] = []\n",
    "        subjects['delta2'] = []\n",
    "        subjects['delta3'] = []\n",
    "        Y['delta1'] = []\n",
    "        Y['delta2'] = []\n",
    "        Y['delta3'] = []\n",
    "        for s in unique_subj:\n",
    "            indeces_temp = np.where(subjects['all']==s)[0]\n",
    "            subject_time[s]= {'t0':None, 't1':None, 't2':None}\n",
    "            for el in indeces_temp:\n",
    "                    if el>lines[1]:\n",
    "                        subject_time[s]['t2']= el\n",
    "                    elif el <= lines[0]:\n",
    "                        subject_time[s]['t0']= el\n",
    "                    else:\n",
    "                        subject_time[s]['t1']= el\n",
    "            if (subject_time[s]['t1'] is not None) and (subject_time[s]['t0'] is not None):\n",
    "                X['delta1'].append((X['all'][subject_time[s]['t1']]-X['all'][subject_time[s]['t0']]).reshape(1, X['all'].shape[1]))\n",
    "                subjects['delta1'].append(s)\n",
    "                Y['delta1'].append(Y['all'][indeces_temp[0]])\n",
    "            if (subject_time[s]['t2'] is not None) and (subject_time[s]['t1'] is not None):\n",
    "                X['delta2'].append((X['all'][subject_time[s]['t2']]-X['all'][subject_time[s]['t1']]).reshape(1, X['all'].shape[1]))\n",
    "                subjects['delta2'].append(s)\n",
    "                Y['delta2'].append(Y['all'][indeces_temp[0]])\n",
    "            if (subject_time[s]['t2'] is not None and subject_time[s]['t0'] is not None):\n",
    "                X['delta3'].append((X['all'][subject_time[s]['t2']]-X['all'][subject_time[s]['t0']]).reshape(1, X['all'].shape[1]))\n",
    "                subjects['delta3'].append(s)\n",
    "                Y['delta3'].append(Y['all'][indeces_temp[0]])\n",
    "        X['delta1'] = np.vstack( X['delta1'])\n",
    "        X['delta2'] = np.vstack( X['delta2'])\n",
    "        X['delta3'] = np.vstack( X['delta3'])\n",
    "        \n",
    "        import xlrd\n",
    " \n",
    "        loc = (file_name)\n",
    "\n",
    "        # To open Workbook\n",
    "        wb = xlrd.open_workbook(loc)\n",
    "        sheet = wb.sheet_by_index(0)\n",
    "\n",
    "        # For row 0 and column 0\n",
    "        IDs = np.array(sheet.col_values(3)[8:])\n",
    "        NetCalc = np.array(sheet.col_values(6)[8:])\n",
    "        frQ = np.array(sheet.col_values(9)[8:])\n",
    "         \n",
    "    return X[mode], np.array(Y[mode]), np.array(subjects[mode]), IDs, NetCalc, frQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for simple classifier:  95.6659998894\n",
      "Time elapsed for whole multilevel:  95.743999958\n",
      "Time elapsed for simple classifier:  97.8040001392\n",
      "Time elapsed for simple classifier:  95.2589998245\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (44,) (45,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-13ef73f432a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m res = full_script(num_folds,num_repeats,scaling, num_permutations, matrix, file_name, \n\u001b[0;32m     52\u001b[0m                                            \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                                            model_params[method], verbose_train, mean)\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;31m#print \"Results: \", res\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-120d7207318e>\u001b[0m in \u001b[0;36mfull_script\u001b[1;34m(num_folds, num_repeats, scaling, num_permutations, par, filename, verbose, ddof, method, feature_selection, num_features, step, model_params, verbose_train, mean)\u001b[0m\n\u001b[0;32m     53\u001b[0m                      \u001b[0mnum_folds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_folds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_repeats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_repeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaling\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                      \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mddof\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_selection\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                      num_features = num_features, step=step, model_params=model_params, verbose_train=verbose_train)\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-67d552a57c9f>\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(X, Y, subj, unique_subj, num_subj, num_unique_subj, num_folds, num_repeats, scaling, par, verbose, ddof, method, feature_selection, num_features, step, model_params, verbose_train)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mauc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mauc_temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0merr_temp\u001b[0m \u001b[1;31m#compute square error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mtrain_error\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtrain_error\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtrain_error_temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m             \u001b[0mtrain_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_auc\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtrain_auc_temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (44,) (45,) "
     ]
    }
   ],
   "source": [
    "file_name = 'OGTT_INPUT.xlsx' #name of the file with data (with path, if not in the same folder)\n",
    "matrix = 'all'          #which matrix to work with(depends on experiment setup)-'between','within','all', 'delta1', 'delta2', 'delta3'           \n",
    "mean = 'delta1'\n",
    "verbose = False                #if True, will plot ROC curves. Else, put False\n",
    "verbose_train = False       #print AUC and error for training data\n",
    "num_comp = 1                  #number of components for PLS\n",
    "method = 'pls'                #which method to use for classifying. 'rf' or 'pls'\n",
    "\n",
    "feature_selection = 'rfe'    #which method to use for feature selection 'rfe', 'rfecv', \"FromModel\" or None\n",
    "num_features = 150           #how many features to select (if method has this parameter)\n",
    "step = 0.3                  #how many features to discard in one iteration, if applicable\n",
    "\n",
    "n_trees = 350 #how many trees there will be in the ensemble classifier Random Forest (if applicable)\n",
    "max_depth = 2   #maximal depth of the decision trees in Random Forest\n",
    "max_features = 1 #maximal number of features concidered at each split by the decision tree in Random Forest\n",
    "min_samples_leaf=1 #minimal number of samples to be left after split (in a leaf) of decision tree in Random Forest\n",
    "\n",
    "C = 1\n",
    "epsilon = 1\n",
    "kernel = 'linear'\n",
    "gamma = 'auto'\n",
    "degree=3\n",
    "\n",
    "scaling = True               #whether to scale the data. True or False  \n",
    "num_folds = 20              #How many folds for k-fold cross-validation\n",
    "num_repeats = 1         #How many times to repeat k-fold cross-validation\n",
    "num_permutations = 0         #How many permutations to make (sanity check for model, \n",
    "                              #if permutated=randomly shuffled data results are too similar - BAD )\n",
    "ddof= 1  #parameter for computing standard deviation,relevant only if scaling = True. \n",
    "\n",
    "model_params = {}\n",
    "model_params['pls'] = {}\n",
    "\n",
    "model_params['pls']['C'] = C\n",
    "model_params['pls']['epsilon'] = epsilon\n",
    "model_params['pls']['kernel'] = kernel\n",
    "model_params['pls']['gamma'] = gamma\n",
    "model_params['pls']['degree'] = degree\n",
    "model_params['pls']['n_trees'] = None\n",
    "model_params['pls']['max_depth'] = None\n",
    "model_params['pls']['max_features'] = None\n",
    "model_params['pls']['min_samples_leaf'] = None\n",
    "model_params['pls']['n_comp'] = 2\n",
    "\n",
    "#res = full_script(num_folds,num_repeats,scaling, num_permutations, matrix, file_name, \n",
    "#                                           verbose, ddof, method, feature_selection, num_features, step, \n",
    "#                                           model_params[method], verbose_train)\n",
    "#print \"Results: \", res\n",
    "\n",
    "matrix = 'all'\n",
    "res = full_script(num_folds,num_repeats,scaling, num_permutations, matrix, file_name, \n",
    "                                           verbose, ddof, method, feature_selection, num_features, step, \n",
    "                                           model_params[method], verbose_train, mean)\n",
    "#print \"Results: \", res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <type 'list'>\n",
      "Type of values are:  <type 'list'>\n"
     ]
    }
   ],
   "source": [
    "b = {'id':[i for i in range(8000)], 'delta1':[0.1 for i in range(8000)], 'delta2':[0.7 for i in range(8000)]}\n",
    "print \"Type: \", type(b['delta1'])\n",
    "dest_file = \"t_stuff.csv\"\n",
    "print \"Type of values are: \", type(b.values())\n",
    "with open(dest_file, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    #writer.writeheader()\n",
    "    writer.writerow(b.keys())\n",
    "    writer.writerows(zip(*b.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22313016014842985"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(np.e, -1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = [27106,\n",
    "105321,\n",
    "126660,\n",
    "115477,\n",
    "105371,\n",
    "141743,\n",
    "111233,\n",
    "124210,\n",
    "143869,\n",
    "123297,\n",
    "145408,\n",
    "98783,\n",
    "110631,\n",
    "116150,\n",
    "144914,\n",
    "114368,\n",
    "55564,\n",
    "108997,\n",
    "151744,\n",
    "61031,\n",
    "144007,\n",
    "130687,\n",
    "70656,\n",
    "143287,\n",
    "111268,\n",
    "143689,\n",
    "133428,\n",
    "116684,\n",
    "56977,\n",
    "167862,\n",
    "114684,\n",
    "176474,\n",
    "38745,\n",
    "113884,\n",
    "118805,\n",
    "101025,\n",
    "99988,\n",
    "77782,\n",
    "137624,\n",
    "133451,\n",
    "146956,\n",
    "146587,\n",
    "147223,\n",
    "151351,\n",
    "147353,\n",
    "53154,\n",
    "157135,\n",
    "131956,\n",
    "117846,\n",
    "109380,\n",
    "148226,\n",
    "114713,\n",
    "111846,\n",
    "105099,\n",
    "41791,\n",
    "143598,\n",
    "160588,\n",
    "160162,\n",
    "149503,\n",
    "111308,\n",
    "135131,\n",
    "121102,\n",
    "99759,\n",
    "118373,\n",
    "140558,\n",
    "133646,\n",
    "150408,\n",
    "144329,\n",
    "147111,\n",
    "25378,\n",
    "26440,\n",
    "122867,\n",
    "81741,\n",
    "120258,\n",
    "133826,\n",
    "114451,\n",
    "123300,\n",
    "156797,\n",
    "143764,\n",
    "173188,\n",
    "72767,\n",
    "140464,\n",
    "133371,\n",
    "55674,\n",
    "101847,\n",
    "46429,\n",
    "158312,\n",
    "148683,\n",
    "117161,\n",
    "102078,\n",
    "107153,\n",
    "165258,\n",
    "134953,\n",
    "134826,\n",
    "117812,\n",
    "161209,\n",
    "172297,\n",
    "97013,\n",
    "138437,\n",
    "113384,\n",
    "64418,\n",
    "101171,\n",
    "133430,\n",
    "129606,\n",
    "69565,\n",
    "144012,\n",
    "147886,\n",
    "106130\n",
    "91957\n",
    "78910\n",
    "168924\n",
    "132854\n",
    "149525\n",
    "142397\n",
    "54469\n",
    "81194\n",
    "152638\n",
    "136217\n",
    "102071\n",
    "109189\n",
    "132690\n",
    "84139\n",
    "57622\n",
    "134992\n",
    "116388\n",
    "46949\n",
    "105895\n",
    "126667\n",
    "123808\n",
    "175673\n",
    "159126\n",
    "159222\n",
    "61572\n",
    "112401\n",
    "62597\n",
    "66586\n",
    "109893\n",
    "112171\n",
    "117133\n",
    "144035\n",
    "93876\n",
    "101797\n",
    "157654\n",
    "156409\n",
    "105832\n",
    "172100\n",
    "75611\n",
    "122104\n",
    "111856\n",
    "76912\n",
    "90322\n",
    "83709\n",
    "99724\n",
    "132331\n",
    "113328\n",
    "108871\n",
    "139402\n",
    "148039\n",
    "114194\n",
    "49544\n",
    "114268\n",
    "106671\n",
    "151593\n",
    "103372\n",
    "144395\n",
    "166516\n",
    "94848\n",
    "41012\n",
    "133631\n",
    "126557\n",
    "87247\n",
    "143847\n",
    "120961\n",
    "83971\n",
    "164989\n",
    "133992\n",
    "148861\n",
    "125532\n",
    "158277\n",
    "81993\n",
    "153432\n",
    "140036\n",
    "123016\n",
    "152261\n",
    "140235\n",
    "115198\n",
    "118088\n",
    "93554\n",
    "144322\n",
    "141866\n",
    "75234\n",
    "116141\n",
    "120674\n",
    "89149\n",
    "174883\n",
    "117044\n",
    "120459\n",
    "153530\n",
    "118013\n",
    "184329\n",
    "133810\n",
    "91913\n",
    "131636\n",
    "161386\n",
    "34067\n",
    "116765\n",
    "116969\n",
    "95618\n",
    "96311\n",
    "70486\n",
    "120259\n",
    "169783\n",
    "108399\n",
    "116452\n",
    "118150\n",
    "104612\n",
    "111945\n",
    "115738\n",
    "86401\n",
    "150517\n",
    "112685\n",
    "151009\n",
    "153224\n",
    "115143\n",
    "56201\n",
    "106307\n",
    "140721\n",
    "86504\n",
    "117583\n",
    "176937\n",
    "145919\n",
    "122478\n",
    "83231\n",
    "137802\n",
    "99860\n",
    "94215\n",
    "120669\n",
    "158755\n",
    "144820\n",
    "78839\n",
    "156017\n",
    "146842\n",
    "106519\n",
    "156284\n",
    "158804\n",
    "98538\n",
    "134658\n",
    "125941\n",
    "111905\n",
    "91894\n",
    "169965\n",
    "145817\n",
    "110192\n",
    "132130\n",
    "129183\n",
    "133553\n",
    "185709\n",
    "114803\n",
    "175066\n",
    "170398\n",
    "24102\n",
    "122442\n",
    "103707\n",
    "157989\n",
    "53126\n",
    "161317\n",
    "178875\n",
    "114201\n",
    "55040\n",
    "72241\n",
    "160226\n",
    "133997\n",
    "120639\n",
    "101234\n",
    "143145\n",
    "148528\n",
    "172174\n",
    "118560\n",
    "170927\n",
    "97619\n",
    "92065\n",
    "94997\n",
    "180769\n",
    "89932\n",
    "45822\n",
    "123011\n",
    "111652\n",
    "117664\n",
    "65937\n",
    "118568\n",
    "114213\n",
    "144951\n",
    "150151\n",
    "120735\n",
    "105372\n",
    "23204\n",
    "116613\n",
    "75714\n",
    "174279\n",
    "119578\n",
    "76415\n",
    "86756\n",
    "122054\n",
    "129522\n",
    "110757\n",
    "104326\n",
    "128750\n",
    "41131\n",
    "141850\n",
    "165086\n",
    "192530\n",
    "122508\n",
    "141075\n",
    "159266\n",
    "152181\n",
    "27836\n",
    "98225\n",
    "147917\n",
    "133929\n",
    "173147\n",
    "100418\n",
    "180201\n",
    "79004\n",
    "139889\n",
    "142471\n",
    "151173\n",
    "91437\n",
    "154395\n",
    "132134\n",
    "124782\n",
    "98351\n",
    "152433\n",
    "138484\n",
    "145664\n",
    "128156\n",
    "156326\n",
    "140495\n",
    "88089\n",
    "160577\n",
    "179103\n",
    "94873\n",
    "152891\n",
    "166110\n",
    "136197\n",
    "134480\n",
    "109826\n",
    "145249\n",
    "156301\n",
    "46468\n",
    "112335\n",
    "114199\n",
    "133993\n",
    "138644\n",
    "103674\n",
    "127758\n",
    "98806\n",
    "138190\n",
    "135112\n",
    "157557\n",
    "122803\n",
    "114526\n",
    "134154\n",
    "144127\n",
    "126225\n",
    "97056\n",
    "120289\n",
    "149273\n",
    "143665\n",
    "143395\n",
    "93477\n",
    "145623\n",
    "133623\n",
    "120761\n",
    "85974\n",
    "94895\n",
    "103675\n",
    "151299\n",
    "120670\n",
    "74704\n",
    "86897\n",
    "174979\n",
    "96732\n",
    "149030\n",
    "100728\n",
    "142786\n",
    "138780\n",
    "153667\n",
    "141582\n",
    "134644\n",
    "124628\n",
    "136238\n",
    "141199\n",
    "157153\n",
    "136635\n",
    "143274\n",
    "147760\n",
    "126393\n",
    "108622\n",
    "138061\n",
    "33607\n",
    "158654\n",
    "165739\n",
    "97581\n",
    "125320\n",
    "134358\n",
    "123793\n",
    "93456\n",
    "123087\n",
    "118349\n",
    "123912\n",
    "115801\n",
    "173199\n",
    "83147\n",
    "112431\n",
    "134281\n",
    "84512\n",
    "171466\n",
    "160346\n",
    "96777\n",
    "134960\n",
    "158015\n",
    "108827\n",
    "121614\n",
    "99130\n",
    "126381\n",
    "96358\n",
    "116874\n",
    "103657\n",
    "156972\n",
    "73158\n",
    "157541\n",
    "98206\n",
    "129885\n",
    "114111\n",
    "111272\n",
    "142284\n",
    "150296\n",
    "153926\n",
    "120293\n",
    "160580\n",
    "108345\n",
    "125429\n",
    "135723\n",
    "118014\n",
    "84867\n",
    "121381\n",
    "99833\n",
    "124392\n",
    "172528\n",
    "106375\n",
    "133737\n",
    "102054\n",
    "106468\n",
    "126611\n",
    "94999\n",
    "151337\n",
    "150153\n",
    "160205\n",
    "88722\n",
    "15941\n",
    "123046\n",
    "124758\n",
    "125737\n",
    "168699\n",
    "131804\n",
    "131037\n",
    "161732\n",
    "136140\n",
    "149649\n",
    "29130\n",
    "133438\n",
    "83051\n",
    "152526\n",
    "112635\n",
    "102982\n",
    "137656\n",
    "110274\n",
    "129979\n",
    "112437\n",
    "92918\n",
    "156122\n",
    "38860\n",
    "104914\n",
    "88233\n",
    "156253\n",
    "95739\n",
    "149189\n",
    "63581\n",
    "117109\n",
    "100764\n",
    "57571\n",
    "136303\n",
    "116092\n",
    "135837\n",
    "118641\n",
    "109664\n",
    "156294\n",
    "93108\n",
    "70020\n",
    "136091\n",
    "174794\n",
    "93015\n",
    "147427\n",
    "140056\n",
    "136815\n",
    "117573\n",
    "102537\n",
    "116193\n",
    "108904\n",
    "123324\n",
    "60576\n",
    "129222\n",
    "156926\n",
    "111866\n",
    "143546\n",
    "145389\n",
    "138660\n",
    "89192\n",
    "115231\n",
    "118862\n",
    "121029\n",
    "163107\n",
    "150305\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[10, 9, 11, 2, 5, 6, 7, 3, 0, 4],[10, 9, 11, 2, 5, 6, 7, 3, 0, 4],[10, 9, 11, 2, 5, 6, 7, 3, 0, 4]]\n",
    "b = [1, 3, 2, 7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
